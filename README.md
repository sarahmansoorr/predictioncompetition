# predictioncompetition
Placed 4th out of 60+ teams on Kaggle competition. 

Explored different models and techniques i.e. Trees, Random Forest, GAMS, Boosting, Splines, BasisExpansions, etc., to select optimal model to predict a large data set. 

Given a set of training data, we discovered different models to see which one had the lowest SSE on the test data. 

The model that did the best was boosting with 5000 trees and lambda 0.01. The model that did the worst was the polynomial using all the predictors. 

Random forests did second best, and bagging did third best. 

The project was a great experience to compare models and be able to see firsthand how drastic the changes for some models are and how similar some models are.
